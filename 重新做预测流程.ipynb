{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/kevin_gwdong/djangoproject/tensorflow/lstmPrediction_twocolumn')\n",
    "\n",
    "from core.data_processor import DataLoader\n",
    "from core.model import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def convert_datetime_to_checkpoint( realdata, datetime):\n",
    "    # 这是针对于有header的打他frame？是这样吗？需要验证\n",
    "    checkpoint = realdata.loc[realdata['Date'] == datetime].index[0]\n",
    "    return checkpoint \n",
    "\n",
    "def convert_checkpoint_to_datetime( realdata, checkpoint):\n",
    "    datetime = realdata.iloc[checkpoint]['Date']\n",
    "    return datetime \n",
    "\n",
    "\n",
    "def normalise_windows(window_data, single_window=False):\n",
    "    '''Normalise window with a base value of zero'''\n",
    "    normalised_data = []\n",
    "    window_data = [window_data] if single_window else window_data\n",
    "    for window in window_data:\n",
    "        normalised_window = []\n",
    "        for col_i in range(window.shape[1]):\n",
    "            normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "            normalised_window.append(normalised_col)\n",
    "        normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "        normalised_data.append(normalised_window)\n",
    "    return np.array(normalised_data)\n",
    "\n",
    "def predict_5_full(startDate, stock):\n",
    "    gc.collect()\n",
    "#     high:\n",
    "    p1_high = predict(startDate, stock, \"High\", \"3\")\n",
    "    p2_high = predict(startDate, stock, \"High\", \"4\")\n",
    "    p3_high = predict(startDate, stock, \"High\", \"5\")\n",
    "    p4_high = predict(startDate, stock, \"High\", \"6\")\n",
    "    p5_high = predict(startDate, stock, \"High\", \"7\")\n",
    "#     low:\n",
    "    p1_low = predict(startDate, stock, \"Low\", \"3\")\n",
    "    p2_low = predict(startDate, stock, \"Low\", \"4\")\n",
    "    p3_low = predict(startDate, stock, \"Low\", \"5\")\n",
    "    p4_low = predict(startDate, stock, \"Low\", \"6\")\n",
    "    p5_low = predict(startDate, stock, \"Low\", \"7\")\n",
    "    \n",
    "    p_5_h = [p1_high, p2_high, p3_high, p4_high, p5_high]\n",
    "    p_5_l = [p1_low, p2_low, p3_low, p4_low, p5_low]\n",
    "    gc.collect()\n",
    "    realdata = pd.read_csv(Path('data/' + stock + '.CSV').resolve())\n",
    "    startPoint = convert_datetime_to_checkpoint(realdata, startDate)\n",
    "    \n",
    "    if realdata.iloc[-1]['Date'] == startDate:\n",
    "        date = \"tomorrow\"\n",
    "    else:\n",
    "        date = realdata.iloc[startPoint+1]['Date']\n",
    "    print(date)   \n",
    "    predictions = {\n",
    "        \"code\":stock,\n",
    "        \"date\":date,\n",
    "        \"high\":p_5_h,\n",
    "        \"low\":p_5_l\n",
    "    }\n",
    "    # file_to_save = \"../predictions_result/\" + stock + \"_\" + column + \"_\" + startDate + \".json\"\n",
    "    file_to_save = Path(\"predictions_result_volume/stock/\" + stock + \"_\" + startDate + \".json\").resolve()\n",
    "    with open(file_to_save, 'w') as f:\n",
    "        json.dump(predictions, f)\n",
    "    print(predictions)\n",
    "    gc.collect()\n",
    "    return predictions\n",
    "\n",
    "'''\n",
    "    {\n",
    "        \"code\":'000738',\n",
    "        'date': '2019-09-10',\n",
    "        \"high\":[13.44409406979382,13.44409406979382,13.44409406979382,13.44409406979382,13.44409406979382],\n",
    "        \"low\":[13.44409406979382,13.44409406979382,13.44409406979382,13.44409406979382,13.44409406979382]\n",
    "    }\n",
    "\n",
    "'''    \n",
    "def predict(startDate, stock, column, sequence_length ) :\n",
    "    # configs = json.load(open('../configs/config.json', 'r'))\n",
    "    # global configs\n",
    "    gc.collect()\n",
    "    configs = json.load(open(Path('configs/config.json').resolve(),'r'))\n",
    "\n",
    "    if column == \"Close\" :\n",
    "        configs['data']['columns'] = [\"Close\", \"Volume\"]\n",
    "    elif column == \"High\" :\n",
    "        configs['data']['columns'] = [\"High\",  \"Volume\"]\n",
    "    elif column == \"Low\" :\n",
    "        configs['data']['columns'] = [\"Low\", \"Volume\"]\n",
    "\n",
    "    configs['data']['filename'] = stock + \".csv\"\n",
    "    configs['data']['sequence_length'] = int(sequence_length)\n",
    "    configs['model']['layers'][0]['input_timesteps'] = int(sequence_length) - 1\n",
    "\n",
    "\n",
    "    # model_save_dir = \"../bestmodels/\" + sequence_length + \"/\" + stock + \"-\" + column + \".h5\"\n",
    "    model_save_dir = Path(\"bestmodels/volume/\" + sequence_length + \"/\" + stock + \"-\" + column + \".h5\").resolve()\n",
    "    # print(model_save_dir)\n",
    "    model = Model()\n",
    "    model.load_model(str(model_save_dir))\n",
    "\n",
    "    # realdata = pd.read_csv('data/' + stock + '.CSV')\n",
    "    # global realdata\n",
    "    realdata = pd.read_csv(Path('data/' + stock + '.CSV').resolve())\n",
    "\n",
    "    startPoint = convert_datetime_to_checkpoint(realdata, startDate)+1\n",
    "\n",
    "    raw_data_windows = realdata.get(configs['data']['columns']).values[startPoint-int(sequence_length)+1:startPoint]\n",
    "\n",
    "    data_windows = np.array(raw_data_windows).astype(float)\n",
    "#     print(data_windows)\n",
    "    data_windows = normalise_windows(data_windows, single_window=True)\n",
    "    prediction = model.predict_point_by_point(data_windows)\n",
    "    prediction = raw_data_windows[0][0]* (1 + prediction[0])\n",
    "    gc.collect()\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有close的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n",
      "2019-09-20\n",
      "600150  already have, check if high low close exist\n",
      "600150  we have high and low, test if we have close\n",
      "600150  we have close, we have everything, can update\n",
      "currently doing:  600150_2019-09-19\n",
      "{'_id': '600150_2019-09-19', 'code': '600150', 'date': '2019-09-20', 'high': [24.44427627597004, 24.595371733233332, 24.19162942968309, 25.429712792634966, 24.639821371547875], 'low': [23.753893183479086, 24.453880053414032, 23.801147470772268, 24.41021479714662, 24.144074062779545], 'close': [24.260704134383705, 24.255610937383025, 24.598400983959436, 25.556029208004475, 24.520337117195595]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_id': '600150_2019-09-19',\n",
       " 'code': '600150',\n",
       " 'date': '2019-09-20',\n",
       " 'high': [24.44427627597004,\n",
       "  24.595371733233332,\n",
       "  24.19162942968309,\n",
       "  25.429712792634966,\n",
       "  24.639821371547875],\n",
       " 'low': [23.753893183479086,\n",
       "  24.453880053414032,\n",
       "  23.801147470772268,\n",
       "  24.41021479714662,\n",
       "  24.144074062779545],\n",
       " 'close': [24.260704134383705,\n",
       "  24.255610937383025,\n",
       "  24.598400983959436,\n",
       "  25.556029208004475,\n",
       "  24.520337117195595]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/kevin_gwdong/djangoproject/tensorflow/lstmPrediction_twocolumn')\n",
    "\n",
    "from core.data_processor import DataLoader\n",
    "from core.model import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def convert_datetime_to_checkpoint( realdata, datetime):\n",
    "    # 这是针对于有header的打他frame？是这样吗？需要验证\n",
    "    checkpoint = realdata.loc[realdata['Date'] == datetime].index[0]\n",
    "    return checkpoint \n",
    "\n",
    "def convert_checkpoint_to_datetime( realdata, checkpoint):\n",
    "    datetime = realdata.iloc[checkpoint]['Date']\n",
    "    return datetime \n",
    "\n",
    "\n",
    "def normalise_windows(window_data, single_window=False):\n",
    "    '''Normalise window with a base value of zero'''\n",
    "    normalised_data = []\n",
    "    window_data = [window_data] if single_window else window_data\n",
    "    for window in window_data:\n",
    "        normalised_window = []\n",
    "        for col_i in range(window.shape[1]):\n",
    "            normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "            normalised_window.append(normalised_col)\n",
    "        normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "        normalised_data.append(normalised_window)\n",
    "    return np.array(normalised_data)\n",
    "\n",
    "def predict_5_full(startDate, stock, conn):\n",
    "    gc.collect()\n",
    "    \n",
    "    predictions = {}    \n",
    "    predictions['_id'] = stock + \"_\" + startDate\n",
    "    db = conn.stock\n",
    "\n",
    "    realdata = pd.read_csv(Path('data/' + stock + '.CSV').resolve())\n",
    "    startPoint = convert_datetime_to_checkpoint(realdata, startDate)\n",
    "    \n",
    "    if realdata.iloc[-1]['Date'] == startDate:\n",
    "        date = \"tomorrow\"\n",
    "    else:\n",
    "        date = realdata.iloc[startPoint+1]['Date']\n",
    "    print(date)       \n",
    "    \n",
    "    if stock in db.list_collection_names() :\n",
    "        print(stock, \" already have, check if high low close exist\")\n",
    "#         try:\n",
    "        query = { \"_id\": predictions['_id'] }\n",
    "        record = db[str(stock)].find_one(query)\n",
    "        if record != None :\n",
    "            print(stock, \" we have high and low, test if we have close\")\n",
    "            if \"close\" in record :\n",
    "                print(stock, \" we have close, we have everything, can update\")\n",
    "                newvalues = {\"$set\":{\n",
    "                \"code\":stock,\n",
    "                \"date\":date,\n",
    "                \"high\":record['high'],\n",
    "                \"low\":record['low'],\n",
    "                \"close\":record['close']\n",
    "                }}\n",
    "                db[str(stock)].update_one(query, newvalues)\n",
    "            else:\n",
    "                print(stock, \" we dont have close\")\n",
    "            # we dont have close, do close work.\n",
    "            #     close:\n",
    "                p1_close = predict(startDate, stock, \"Close\", \"3\")\n",
    "                p2_close = predict(startDate, stock, \"Close\", \"4\")\n",
    "                p3_close = predict(startDate, stock, \"Close\", \"5\")\n",
    "                p4_close = predict(startDate, stock, \"Close\", \"6\")\n",
    "                p5_close = predict(startDate, stock, \"Close\", \"7\")\n",
    "                p_5_c = [p1_close, p2_close, p3_close, p4_close, p5_close]\n",
    "                newvalues = {\"$set\":{\n",
    "                \"code\":stock,\n",
    "                \"date\":date,\n",
    "                \"high\":record['high'],\n",
    "                \"low\":record['low'],\n",
    "                \"close\":p_5_c\n",
    "                }}\n",
    "                db[str(stock)].update_one(query, newvalues)\n",
    "        else:\n",
    "            print(stock, \" this day's record not exist we try to predict all\")                \n",
    "        #     high:\n",
    "            p1_high = predict(startDate, stock, \"High\", \"3\")\n",
    "            p2_high = predict(startDate, stock, \"High\", \"4\")\n",
    "            p3_high = predict(startDate, stock, \"High\", \"5\")\n",
    "            p4_high = predict(startDate, stock, \"High\", \"6\")\n",
    "            p5_high = predict(startDate, stock, \"High\", \"7\")\n",
    "        #     low:\n",
    "            p1_low = predict(startDate, stock, \"Low\", \"3\")\n",
    "            p2_low = predict(startDate, stock, \"Low\", \"4\")\n",
    "            p3_low = predict(startDate, stock, \"Low\", \"5\")\n",
    "            p4_low = predict(startDate, stock, \"Low\", \"6\")\n",
    "            p5_low = predict(startDate, stock, \"Low\", \"7\")\n",
    "        #     close:\n",
    "            p1_close = predict(startDate, stock, \"Close\", \"3\")\n",
    "            p2_close = predict(startDate, stock, \"Close\", \"4\")\n",
    "            p3_close = predict(startDate, stock, \"Close\", \"5\")\n",
    "            p4_close = predict(startDate, stock, \"Close\", \"6\")\n",
    "            p5_close = predict(startDate, stock, \"Close\", \"7\")\n",
    "\n",
    "            p_5_h = [p1_high, p2_high, p3_high, p4_high, p5_high]\n",
    "            p_5_l = [p1_low, p2_low, p3_low, p4_low, p5_low]\n",
    "            p_5_c = [p1_close, p2_close, p3_close, p4_close, p5_close]\n",
    "\n",
    "            predictions = {\n",
    "                '_id':stock + \"_\" + startDate,\n",
    "                \"code\":stock,\n",
    "                \"date\":date,\n",
    "                \"high\":p_5_h,\n",
    "                \"low\":p_5_l,\n",
    "                \"close\":p_5_c\n",
    "            }        \n",
    "            db[str(stock)].insert_one(predictions)\n",
    "#         except:\n",
    "#             print(\"something wrong happen!\")\n",
    "    else:\n",
    "        print(stock, \" we dont have collection, creat one and predict\")\n",
    "        db.create_collection(stock)            \n",
    "    #     high:\n",
    "        p1_high = predict(startDate, stock, \"High\", \"3\")\n",
    "        p2_high = predict(startDate, stock, \"High\", \"4\")\n",
    "        p3_high = predict(startDate, stock, \"High\", \"5\")\n",
    "        p4_high = predict(startDate, stock, \"High\", \"6\")\n",
    "        p5_high = predict(startDate, stock, \"High\", \"7\")\n",
    "    #     low:\n",
    "        p1_low = predict(startDate, stock, \"Low\", \"3\")\n",
    "        p2_low = predict(startDate, stock, \"Low\", \"4\")\n",
    "        p3_low = predict(startDate, stock, \"Low\", \"5\")\n",
    "        p4_low = predict(startDate, stock, \"Low\", \"6\")\n",
    "        p5_low = predict(startDate, stock, \"Low\", \"7\")\n",
    "    #     close:\n",
    "        p1_close = predict(startDate, stock, \"Close\", \"3\")\n",
    "        p2_close = predict(startDate, stock, \"Close\", \"4\")\n",
    "        p3_close = predict(startDate, stock, \"Close\", \"5\")\n",
    "        p4_close = predict(startDate, stock, \"Close\", \"6\")\n",
    "        p5_close = predict(startDate, stock, \"Close\", \"7\")\n",
    "\n",
    "        p_5_h = [p1_high, p2_high, p3_high, p4_high, p5_high]\n",
    "        p_5_l = [p1_low, p2_low, p3_low, p4_low, p5_low]\n",
    "        p_5_c = [p1_close, p2_close, p3_close, p4_close, p5_close]\n",
    "\n",
    "        predictions = {\n",
    "            \"_id\":stock + \"_\" + startDate,\n",
    "            \"code\":stock,\n",
    "            \"date\":date,\n",
    "            \"high\":p_5_h,\n",
    "            \"low\":p_5_l,\n",
    "            \"close\":p_5_c\n",
    "        }         \n",
    "        db[str(stock)].insert_one(predictions)\n",
    "        \n",
    "        \n",
    "    query = { \"_id\": predictions['_id'] }\n",
    "    predictions = db[str(stock)].find_one(query)\n",
    "    print(\"currently doing: \",predictions['_id'])\n",
    "    print(predictions)    \n",
    "    print()\n",
    "    \n",
    "    return predictions\n",
    "  \n",
    "def predict(startDate, stock, column, sequence_length ) :\n",
    "    # configs = json.load(open('../configs/config.json', 'r'))\n",
    "    # global configs\n",
    "    gc.collect()\n",
    "    configs = json.load(open(Path('configs/config.json').resolve(),'r'))\n",
    "\n",
    "    if column == \"Close\" :\n",
    "        configs['data']['columns'] = [\"Close\", \"Volume\"]\n",
    "    elif column == \"High\" :\n",
    "        configs['data']['columns'] = [\"High\",  \"Volume\"]\n",
    "    elif column == \"Low\" :\n",
    "        configs['data']['columns'] = [\"Low\", \"Volume\"]\n",
    "\n",
    "    configs['data']['filename'] = stock + \".csv\"\n",
    "    configs['data']['sequence_length'] = int(sequence_length)\n",
    "    configs['model']['layers'][0]['input_timesteps'] = int(sequence_length) - 1\n",
    "\n",
    "\n",
    "    # model_save_dir = \"../bestmodels/\" + sequence_length + \"/\" + stock + \"-\" + column + \".h5\"\n",
    "    model_save_dir = Path(\"bestmodels/volume/\" + sequence_length + \"/\" + stock + \"-\" + column + \".h5\").resolve()\n",
    "    # print(model_save_dir)\n",
    "    model = Model()\n",
    "    model.load_model(str(model_save_dir))\n",
    "\n",
    "    # realdata = pd.read_csv('data/' + stock + '.CSV')\n",
    "    # global realdata\n",
    "    realdata = pd.read_csv(Path('data/' + stock + '.CSV').resolve())\n",
    "\n",
    "    startPoint = convert_datetime_to_checkpoint(realdata, startDate)+1\n",
    "\n",
    "    raw_data_windows = realdata.get(configs['data']['columns']).values[startPoint-int(sequence_length)+1:startPoint]\n",
    "\n",
    "    data_windows = np.array(raw_data_windows).astype(float)\n",
    "#     print(data_windows)\n",
    "    data_windows = normalise_windows(data_windows, single_window=True)\n",
    "    prediction = model.predict_point_by_point(data_windows)\n",
    "    prediction = raw_data_windows[0][0]* (1 + prediction[0])\n",
    "    gc.collect()\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "#     parser.add_argument(\"--predict_one\", \"-po\", help=\"start date\")\n",
    "    parser.add_argument(\"--start_date\", \"-sd\", help=\"start date\")\n",
    "#     parser.add_argument(\"--end_date\", \"-ed\", help=\"end date\")\n",
    "    parser.add_argument(\"--stockcode\", \"-sc\", help=\"stock code name\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try: \n",
    "        conn = MongoClient() \n",
    "        print(\"Connected successfully!!!\") \n",
    "    except:   \n",
    "        print(\"Could not connect to MongoDB\") \n",
    "\n",
    "    startDate = args.start_date\n",
    "    stock = args.stockcode\n",
    "    predict_5_full(startDate, stock, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # print(stock)\n",
    "#     realdata = pd.read_csv(Path('data/' + stock + '.CSV').resolve())\n",
    "    \n",
    "#     if args.predict_one == \"true\":\n",
    "#         print(startDate)\n",
    "#         id= stock + \"_\" + date \n",
    "#         db = conn.stock\n",
    "#         query = { \"_id\": id }\n",
    "#         data = db[str(stock)].find_one(query)\n",
    "#         if data == None :\n",
    "#             predict_5_full(date, stock, conn)\n",
    "#             print(date,\" we don't have this data, start predict\")\n",
    "#         elif data['date'] == \"tomorrow\" :\n",
    "#             print(date, \" data is tomorrow, we need to re-run this\")\n",
    "#             predict_5_full(date, stock, conn)\n",
    "#         else:\n",
    "#             print(date, \"we have this data, no need to do again\")\n",
    "\n",
    "#         # predict_5_full(startDate, stock, conn)\n",
    "#     else:\n",
    "#         enddate = args.end_date\n",
    "#         data_selected = realdata.loc[(realdata['Date'] >= startDate) & (realdata['Date'] <= enddate)]['Date'].values\n",
    "#         print(data_selected)\n",
    "        \n",
    "#         for date in data_selected:\n",
    "#             gc.collect()\n",
    "#             id= stock + \"_\" + date \n",
    "#             db = conn.stock\n",
    "#             query = { \"_id\": id }\n",
    "#             data = db[str(stock)].find_one(query)\n",
    "#             if data == None :\n",
    "#                 print(date, \" we don't have this data, start predict\")\n",
    "#                 predict_5_full(date, stock, conn)\n",
    "#             elif data['date'] == \"tomorrow\" :\n",
    "#                 print(date, \" data is tomorrow, we need to re-run this\")\n",
    "#                 predict_5_full(date, stock, conn)\n",
    "#             else:\n",
    "#                 print(date, \" we have this data, no need to do again\")\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "\n",
    "# try: \n",
    "#     conn = MongoClient() \n",
    "#     print(\"Connected successfully!!!\") \n",
    "# except:   \n",
    "#     print(\"Could not connect to MongoDB\")  \n",
    "\n",
    "# stock = '600150'\n",
    "# startDate = '2019-09-19'\n",
    "# predict_5_full(startDate, stock, conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-12-10' '2019-12-11' '2019-12-12' '2019-12-13' '2019-12-16'\n",
      " '2019-12-17' '2019-12-18' '2019-12-19']\n",
      "[array(['2019-12-10', '2019-12-11', '2019-12-12', '2019-12-13'],\n",
      "      dtype=object), array(['2019-12-16', '2019-12-17', '2019-12-18', '2019-12-19'],\n",
      "      dtype=object), array([], dtype=object)]\n",
      "now we are doing this 2019-12-10\n",
      "2019-12-11\n",
      "{'code': '600150', 'date': '2019-12-11', 'high': [20.808384922025727, 20.867798846345394, 20.56043630269356, 20.56492784384638, 21.07589416205883], 'low': [20.37528644882841, 20.071784756274425, 20.318304409720003, 19.801973994262514, 20.546668966300786], 'close': [20.674021056685596, 20.509540706034752, 20.598980691656468, 20.7365649786219, 20.389949499815703]}\n",
      "{'code': '600150', 'date': '2019-12-11', 'high': [20.808384922025727, 20.867798846345394, 20.56043630269356, 20.56492784384638, 21.07589416205883], 'low': [20.37528644882841, 20.071784756274425, 20.318304409720003, 19.801973994262514, 20.546668966300786], 'close': [20.674021056685596, 20.509540706034752, 20.598980691656468, 20.7365649786219, 20.389949499815703]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fffecb3fd290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"now we are doing this\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpredict_5_full_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtime_init\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtime_init\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6f04057da9d9>\u001b[0m in \u001b[0;36mpredict_5_full_close\u001b[0;34m(startDate, stock)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstartDate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;31m#     if stock in db.list_collection_names() :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#         # print(stock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "# \n",
    "# startDate = '2019-09-19'\n",
    "stock = '600150' # 中国船舶\n",
    "\n",
    "\n",
    "# predict(startDate, stock, column, sequence_length )\n",
    "realdata = pd.read_csv('data/' + stock + '.CSV')\n",
    "data_selected = realdata.loc[(realdata['Date'] > '2019-12-09') & (realdata['Date'] < '2019-12-20')]['Date'].values\n",
    "print(data_selected)\n",
    "\n",
    "# predict_5_full('2019-09-18', stock)\n",
    "# code_list = []\n",
    "\n",
    "# for date in data_selected:\n",
    "# #     gc.collect()\n",
    "# #     predict_5_full(date, stock)\n",
    "#     code_list.append(code)\n",
    "\n",
    "# try: \n",
    "#     conn = MongoClient() \n",
    "#     print(\"Connected successfully!!!\") \n",
    "# except:   \n",
    "#     print(\"Could not connect to MongoDB\")     \n",
    "    \n",
    "new_list = [data_selected[i*4:i*4+4] for i in range(int(len(data_selected)/4+1))]\n",
    "# new_list = [[603721, 603718, 603665, 603578], [603229, 603108, 601698, 600862], [600720, 600703, 600671, 600660], [600637, 600597, 600377, 600298], [600196, 600079, 600060, 600004]]\n",
    "print(new_list)\n",
    "time_init = 300\n",
    "for group in new_list:\n",
    "    for date in group :\n",
    "        print(\"now we are doing this\",date)\n",
    "        predict_5_full_close(date, stock, conn)\n",
    "    time.sleep(time_init)\n",
    "    time_init= time_init+120\n",
    "    print()     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "subprocess.call(shlex.split('./run_close.sh 600150'))\n",
    "subprocess.call(shlex.split('./run_close.sh 002609'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-12-10' '2019-12-11' '2019-12-12' '2019-12-13' '2019-12-16'\n",
      " '2019-12-17' '2019-12-18' '2019-12-19']\n",
      "2019-12-11\n",
      "{'code': '600273', 'date': '2019-12-11', 'high': [9.881097362190484, 9.902788590397686, 9.874099856987597, 9.964093058109283, 9.894068744732067], 'low': [9.693595439773054, 9.698658611699939, 9.720417498052122, 9.662598345130682, 9.70999858884141]}\n",
      "2019-12-12\n",
      "{'code': '600273', 'date': '2019-12-12', 'high': [10.156606001257897, 10.211906186118721, 10.150695068649947, 10.103415874298662, 10.129572808295489], 'low': [9.857550792843105, 9.875090418923646, 9.864542255736888, 9.818207073770465, 9.772461238410324]}\n",
      "2019-12-13\n",
      "{'code': '600273', 'date': '2019-12-13', 'high': [10.079748021438718, 10.056038220226764, 10.079683845117687, 10.031971247456967, 10.03592391544953], 'low': [9.873303701682017, 9.873474586978555, 9.877760065728799, 9.827440265826882, 9.83272573715076]}\n",
      "2019-12-16\n",
      "{'code': '600273', 'date': '2019-12-16', 'high': [10.097876970358193, 10.114083824157714, 10.089639934301376, 10.102556613273919, 10.045381127956789], 'low': [9.988036688044668, 9.99373038796708, 9.985546990558506, 9.935351686393842, 9.949107822999357]}\n",
      "2019-12-17\n",
      "{'code': '600273', 'date': '2019-12-17', 'high': [10.407830720059573, 10.377657874524594, 10.376431093961, 10.253242180347442, 10.336807317472994], 'low': [10.142378411255777, 10.151420402750373, 10.121022134833039, 10.049732574820519, 10.068510703034699]}\n",
      "2019-12-18\n",
      "{'code': '600273', 'date': '2019-12-18', 'high': [10.38943398755975, 10.353493699394166, 10.357176134884357, 10.342885029539465, 10.281872751712799], 'low': [10.161010359935462, 10.15856885947287, 10.162307744175196, 10.076967001128942, 10.118939863890411]}\n",
      "2019-12-19\n",
      "{'code': '600273', 'date': '2019-12-19', 'high': [10.755166826918723, 10.728005208075047, 10.690160194300114, 10.61099183857441, 10.634812028706072], 'low': [10.296455341279508, 10.306884906515478, 10.273778755031525, 10.212375981211663, 10.224454226866365]}\n",
      "2019-12-20\n",
      "{'code': '600273', 'date': '2019-12-20', 'high': [10.822227048128843, 10.785887297987939, 10.760071695297956, 10.719214052520693, 10.693240383267403], 'low': [10.533800172507762, 10.531820736527443, 10.49113579377532, 10.41238470505923, 10.391486070454121]}\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# startDate = '2019-09-19'\n",
    "stock = '600150'\n",
    "\n",
    "\n",
    "# predict(startDate, stock, column, sequence_length )\n",
    "realdata = pd.read_csv('data/' + stock + '.CSV')\n",
    "data_selected = realdata.loc[(realdata['Date'] > '2019-12-09') & (realdata['Date'] < '2019-12-20')]['Date'].values\n",
    "print(data_selected)\n",
    "\n",
    "# predict_5_full('2019-09-18', stock)\n",
    "\n",
    "for date in data_selected:\n",
    "    gc.collect()\n",
    "    predict_5_full(date, stock)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-10\n",
      "{'code': '600273', 'date': '2019-12-10', 'high': [10.23231404332444, 10.218491841536016, 10.206399797201156, 10.233278625011444, 10.158531320095063], 'low': [9.837769505884498, 9.841812307015061, 9.805731458580121, 9.73402933228761, 9.787616497278213]}\n",
      "2019-12-23\n",
      "{'code': '600273', 'date': '2019-12-23', 'high': [10.736831674352288, 10.795922256819903, 10.684115947186948, 10.7428860925138, 10.652130166143179], 'low': [10.382286494411527, 10.370398411955684, 10.380559927076101, 10.312631696537137, 10.387824980542064]}\n",
      "2019-12-27\n",
      "{'code': '600273', 'date': '2019-12-27', 'high': [11.989959039613604, 11.821040415763855, 11.888188874050975, 11.543336151838302, 11.715429595112802], 'low': [10.935366724431516, 10.922169032692908, 10.951446157693862, 10.835131924003363, 10.871518766432999]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'code': '600273',\n",
       " 'date': '2019-12-27',\n",
       " 'high': [11.989959039613604,\n",
       "  11.821040415763855,\n",
       "  11.888188874050975,\n",
       "  11.543336151838302,\n",
       "  11.715429595112802],\n",
       " 'low': [10.935366724431516,\n",
       "  10.922169032692908,\n",
       "  10.951446157693862,\n",
       "  10.835131924003363,\n",
       "  10.871518766432999]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock = '600273'\n",
    "\n",
    "predict_5_full('2019-12-09', stock)\n",
    "# predict_5_full('2019-12-19', stock)\n",
    "predict_5_full('2019-12-20', stock)\n",
    "\n",
    "predict_5_full('2019-12-26', stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/Users/kevin_gwdong/djangoproject/tensorflow/lstmPrediction_twocolumn/bestmodels/volume/3/603998-High.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4ecdd4230d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# predict_5_full('2019-12-30', stock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# predict_5_full('2020-01-06', stock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredict_5_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2019-11-20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2114d459daef>\u001b[0m in \u001b[0;36mpredict_5_full\u001b[0;34m(startDate, stock)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#     high:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mp1_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"High\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mp2_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"High\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mp3_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"High\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2114d459daef>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(startDate, stock, column, sequence_length)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# print(model_save_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# realdata = pd.read_csv('data/' + stock + '.CSV')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/djangoproject/tensorflow/lstmPrediction_twocolumn/core/model.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;31m# print('[Model] Loading model from file %s' % filepath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/djangoproject/tensorflow/tensorenv/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/djangoproject/tensorflow/tensorenv/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/djangoproject/tensorflow/tensorenv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/djangoproject/tensorflow/tensorenv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/Users/kevin_gwdong/djangoproject/tensorflow/lstmPrediction_twocolumn/bestmodels/volume/3/603998-High.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "stock = '603998'\n",
    "\n",
    "# predict_5_full('2019-12-30', stock)\n",
    "# predict_5_full('2020-01-06', stock)\n",
    "predict_5_full('2019-11-20', stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
